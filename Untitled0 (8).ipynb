{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Assignment Question\n",
        "\n",
        "Ans1:- Simple linear regression is a statistical method that uses a straight line to show the relationship between two variables. It's used to estimate the correlation between the variables and to predict values.\n",
        "\n",
        "Ans2:- Assumptions for Simple Linear Regression\n",
        "Linearity: The relationship between and must be linear. ...\n",
        "Independence of errors: There is not a relationship between the residuals and the variable; in other words, is independent of errors. ...\n",
        "Normality of errors: The residuals must be approximately normally distributed.\n",
        "\n",
        "Ans3:- The equation of a straight line is y=mx+c y = m x + c m is the gradient and c is the height at which the line crosses the y -axis, also known as the y -intercept.\n",
        "\n",
        "Ans4:- The value of c in the equation y = mx + c represents the y-intercept of the line. The intercept is the distance from the origin on the y-axis, where this line cuts the y-axis.\n",
        "\n",
        "Ans5:- The formula to calculate the slope (m) of the best fit line in linear regression is obtained by dividing the sum of the products of the differences between the observed values and their means by the sum of the squared differences between the independent variable values and their mean.\n",
        "\n",
        "Ans6:- The least squares method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points.\n",
        "\n",
        "Ans7:- The coefficient of determination (R²) is a number between 0 and 1 that measures how well a statistical model predicts an outcome. You can interpret the R² as the proportion of variation in the dependent variable that is predicted by the statistical model.\n",
        "\n",
        "Ans8:- Multiple linear regression refers to a statistical technique that uses two or more independent variables to predict the outcome of a dependent variable. The technique enables analysts to determine the variation of the model and the relative contribution of each independent variable in the total variance.\n",
        "\n",
        "Ans9:- The main difference between simple and multiple regression is that multiple regression includes two or more independent variables – sometimes called predictor variables – in the model, rather than just one.\n",
        "\n",
        "Ans10:- Key Assumptions of Linear Regression\n",
        "Linearity.\n",
        "Homoscedasticity of Residuals in Linear Regression.\n",
        "Multivariate Normality – Normal Distribution.\n",
        "Lack of Multicollinearity.\n",
        "Absence of Endogeneity.\n",
        "\n",
        "Ans11:- Heteroskedasticity is usually defined as some variation of the phrase “non-constant error variance”, or the idea that, once the predictors have been included in the regression model, the remaining residual variability changes as a function of something that is not in the model (Cohen, West, & Aiken, 2007; Field, 2009\n",
        "\n",
        "Ans12:- To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information.\n",
        "\n",
        "Ans13:- How To Transform Categorical Variables To Numeric Python?\n",
        "Creates dictionary and converts it into a dataframe.\n",
        "Uses \"get_dummies\" function for the encoding.\n",
        "Concats the final encoded dataset into the final dataframe.\n",
        "Drops categorical variable column.\n",
        "\n",
        "Ans14:- An important, and often forgotten, concept in regression analysis is that of interaction terms. In short, interaction terms enable you to examine whether the relationship between the target and the independent variable changes depending on the value of another independent variable.\n",
        "\n",
        "Ans15:- The intercept is the expected value of the response variable when all predictors equal zero. The slope coefficient on each predictor is the expected difference in the outcome variable for a one-unit increase of the predictor, holding all other predictors constant.\n",
        "\n",
        "Ans16:- The slope of regression in species-area relationship predicts species richness of an area. It indicates the dependency of species richness on the area as higher slope reflects higher dependency of the area. Taking into account of a large area, such as country, the slope is almost linear with the area.\n",
        "\n",
        "Ans17:- The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero. In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero.\n",
        "\n",
        "Ans18:- R-squared will give you an estimate of the relationship between movements of a dependent variable based on an independent variable's movements. However, it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased.\n",
        "\n",
        "Ans19:- A smaller standard error suggests that the estimate of the coefficient is more likely to be close to the true population value. In contrast, a larger standard error indicates more variability and less confidence in the precision of the coefficient estimate.\n",
        "\n",
        "Ans20:- Visually, if there appears to be a fan or cone shape in the residual plot, it indicates the presence of heteroskedasticity. Also, regressions with heteroskedasticity show a pattern where the variance of the residuals increases along with the fitted values.\n",
        "\n",
        "Ans21:- While r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance.\n",
        "\n",
        "Ans22:- This helps the model understand how both features affect the outcome, leading to more accurate predictions. So, scaling is important to ensure that all features contribute fairly to the model's learning process.\n",
        "\n",
        "Ans23:-Polynomial regression is a kind of linear regression in which the relationship shared between the dependent and independent variables Y and X is modeled as the nth degree of the polynomial. This is done to look for the best way of drawing a line using data points. Keep reading to know more about polynomial regression.\n",
        "\n",
        "Ans24:- Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Unlike linear regression, polynomial regression can fit non-linear relationships between variables.\n",
        "\n",
        "Ans25:- Polynomial regression is widely used in various fields, including economics, finance, physics, and engineering. It is particularly useful when analyzing complex systems with nonlinear relationships between variables. One common application of polynomial regression is in the analysis of stock prices.\n",
        "\n",
        "Ans26:- You are using polynomial regression when you predict Y using a single X variable together with some of its powers (X2, X3, etc.). Let us consider just the case of X with X2. With these variables, the usual multiple regression equation, Y = a + b1X1 + b2X2, becomes the quadratic polynomial Y = a + b1X + b2X2.\n",
        "\n",
        "Ans27:- What is Multivariate Polynomial Regression? Multivariate polynomial regression is an extension of linear regression that allows for multiple input variables and non-linear relationships between the input variables and the target variable.\n",
        "\n",
        "Ans28:- Overfitting: Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses generalization ability. Careful model selection and regularization techniques are required to mitigate this risk.\n",
        "\n",
        "Ans29:- The solution is to have a separate validation set on which we can evaluate the model for each polynomial degree. If we fit the model using the original data (training set) but evaluate it using the validation set, the model will overfit to the noise in the training set but not in the validation set.\n",
        "\n",
        "Ans30:- A broad range of functions can be fit under it. Polynomial basically fits a wide range of curvatures. Polynomial provides the best approximation of the relationship between dependent and independent variables.\n",
        "\n",
        "Ans31:- Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modelled as an nth-degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y | x). In this article, we’ll go in-depth about polynomial regression."
      ],
      "metadata": {
        "id": "c8ppQ3SoknW2"
      }
    }
  ]
}